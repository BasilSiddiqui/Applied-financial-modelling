{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d7d46cb7-66a9-4a82-96d1-4be2c4b73163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Download data\n",
    "tickers = ['CSCO', 'TMUS', '^GSPC', '^VIX']\n",
    "shares = yf.download(tickers, start='2016-01-01', end='2024-12-31')['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd46481-4e52-470e-8958-d30b836338ae",
   "metadata": {},
   "source": [
    "The dataset comprises daily closing prices for Cisco Systems (CSCO), T-Mobile US (TMUS), S&P 500 Index (^GSPC), and CBOE Volatility Index (^VIX) from January 2016 to December 2024. This selection provides a diversified perspective across different market segments and sectors. CSCO represents the established technology infrastructure sector, offering insights into enterprise networking and communications equipment demand. TMUS provides exposure to the competitive wireless telecommunications services industry, reflecting consumer-facing technology adoption trends.\n",
    "\n",
    "The S&P 500 serves as the broad market benchmark, enabling relative performance analysis between individual stocks and the overall market. The VIX index captures market volatility expectations, allowing examination of how different volatility regimes impact these securities. This combination is particularly valuable as it contrasts a mature tech hardware company (CSCO) with a growth-oriented telecom service provider (TMUS), while using the S&P 500 as a performance reference point and the VIX as a risk sentiment indicator.\n",
    "\n",
    "The selected time frame encompasses several distinct market environments, including the pre-pandemic growth period, COVID-19 market turbulence, the subsequent recovery, and recent high-interest rate conditions. This enables robust analysis of how these securities behave across different market cycles and volatility environments, making the dataset suitable for studying sectoral performance, volatility transmission, and market-relative stock behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "598485d8-bcea-4637-8a52-e43d983b696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>19.789375</td>\n",
       "      <td>38.089603</td>\n",
       "      <td>2012.660034</td>\n",
       "      <td>20.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>19.699459</td>\n",
       "      <td>39.331551</td>\n",
       "      <td>2016.709961</td>\n",
       "      <td>19.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>19.489653</td>\n",
       "      <td>39.165306</td>\n",
       "      <td>1990.260010</td>\n",
       "      <td>20.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>19.040071</td>\n",
       "      <td>39.615139</td>\n",
       "      <td>1943.089966</td>\n",
       "      <td>24.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>18.568003</td>\n",
       "      <td>38.999065</td>\n",
       "      <td>1922.030029</td>\n",
       "      <td>27.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker       Date       CSCO       TMUS        ^GSPC       ^VIX\n",
       "0      2016-01-04  19.789375  38.089603  2012.660034  20.700001\n",
       "1      2016-01-05  19.699459  39.331551  2016.709961  19.340000\n",
       "2      2016-01-06  19.489653  39.165306  1990.260010  20.590000\n",
       "3      2016-01-07  19.040071  39.615139  1943.089966  24.990000\n",
       "4      2016-01-08  18.568003  38.999065  1922.030029  27.010000"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "# Create clean dataframe\n",
    "df_prices = shares.copy().reset_index()\n",
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab602a8-a3c6-4feb-840f-e905d73347c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "import matplotlib.pyplot as plt\n",
    "#We plot the graphs separately as they are of different magnitudes.\n",
    "plot1 = plt.figure(1, figsize=(16, 8), dpi=60)\n",
    "plt.plot(df_prices['CSCO'], color='blue', label='JNJ share price')\n",
    "plt.title('Cisco share price over time')\n",
    "plt.legend()\n",
    "plt.ylabel('Closing Price')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plot2 = plt.figure(2, figsize=(16, 8), dpi=60)\n",
    "plt.plot(df_prices['TMUS'], color='green', label='PG share price')\n",
    "plt.title('Tmobile price over time')\n",
    "plt.legend()\n",
    "plt.ylabel('Closing Price')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plot3 = plt.figure(3, figsize=(16, 8), dpi=60)\n",
    "plt.plot(df_prices['^GSPC'], color='yellow', label='S&P 500 index price')\n",
    "plt.title('S&P 500 index price over time')\n",
    "plt.legend() \n",
    "plt.ylabel('Closing Price')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plot4 = plt.figure(4, figsize=(16, 8), dpi=60)\n",
    "plt.plot(df_prices['^VIX'], color='blue', label='VIX index price')\n",
    "plt.title('VIX index price over time')\n",
    "plt.legend()\n",
    "plt.ylabel('Closing Price')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b50da3-01e5-4ffb-94ae-1190b8bdc65e",
   "metadata": {},
   "source": [
    "Prices for Cisco (blue) and T-Mobile (orange) are plotted against the S&P 500 (green) in the upper plot. Cisco exhibits greater stability, whereas T-Mobile appears to be growing more rapidly, especially after 2019. Both follow the overall trend of the S&P 500, but their volatility varies. Market stress periods, particularly the March 2020 spike, are visible in the lower VIX plot. The division into two subplots, with VIX as an index and stocks/S&P in dollars, manages the various scales well. Price movements and volatility regimes can be directly compared thanks to the transparent timeline and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89fd0b-97dc-4f0b-8611-3a48b1769df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "df_prices.to_csv('stock_prices_data.csv', index=False) #We save the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6ffd9-4fa8-4c92-b512-56d04270746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "import numpy as np\n",
    "#We calculate the differenced log returns for the first 3 variables. Specify the argument np.log, then call the diff() method.\n",
    "csco_ret = df_prices['CSCO'].apply(np.log).diff(1)\n",
    "print(csco_ret)\n",
    "\n",
    "tm_ret = df_prices['TMUS'].apply(np.log).diff()\n",
    "print(tm_ret)\n",
    "\n",
    "gspc_ret = df_prices['^GSPC'].apply(np.log).diff(1)\n",
    "print(gspc_ret)\n",
    "\n",
    "#We calculate the first difference for the VIX index\n",
    "vix_diff = df_prices['^VIX'].diff()\n",
    "print(vix_diff)\n",
    "\n",
    "# Next we create a dictionary called dataset\n",
    "dataset={'CSCO': csco_ret, 'TMUS': tm_ret, '^GSPC': gspc_ret, '^VIX': vix_diff}\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "# Remove missing values\n",
    "dataset = dataset.dropna(how='any')\n",
    "\n",
    "# Display the cleaned dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c65105-d65e-4f3b-9b7f-bc6c41bb576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# create a variable result: call the adfuller() method and specify the 'JNJ' column\n",
    "result = adfuller(dataset['CSCO'])\n",
    "print(f'Test Statistics: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'critical_values: {result[4]}')\n",
    "if result[1] > 0.05:\n",
    "    print(\"Series is not stationary.\")\n",
    "else:\n",
    "    print(\"Series is stationary for Cisco\")\n",
    "\n",
    "\n",
    "# create a variable result: call the adfuller() method and specify the 'PG' column\n",
    "result = adfuller(dataset['TMUS'])\n",
    "print(f'Test Statistics: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'critical_values: {result[4]}')\n",
    "if result[1] > 0.05:\n",
    "    print(\"Series is not stationary.\")\n",
    "else:\n",
    "    print(\"Series is stationary for T-mobile\")\n",
    "\n",
    "\n",
    "# create a variable result: call the adfuller() method and specify the '^GSPC' column\n",
    "result = adfuller(dataset['TMUS'])\n",
    "print(f'Test Statistics: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'critical_values: {result[4]}')\n",
    "if result[1] > 0.05:\n",
    "    print(\"Series is not stationary.\")\n",
    "else:\n",
    "    print(\"Series is stationary for the S&P 500 index (^GSPC)\")\n",
    "\n",
    "\n",
    "# create a variable result: call the adfuller() method and specify the '^VIX' column\n",
    "result = adfuller(dataset['TMUS'])\n",
    "print(f'Test Statistics: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'critical_values: {result[4]}')\n",
    "if result[1] > 0.05:\n",
    "    print(\"Series is not stationary.\")\n",
    "else:\n",
    "    print(\"Series is stationary for the volatility index (^VIX)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e755c-2553-43b2-9502-1739d00546f0",
   "metadata": {},
   "source": [
    "The results of the stationarity test provide compelling evidence that the volatility index (^VIX), S&P 500 (^GSPC) are all stationary.  At all confidence levels (1%, 5%, and 10%), the test statistics are substantially more negative than the critical values, ranging from -11.38 for JNJ to -15.31 for the other series.  The incredibly low p-values (ranging from 8.49e-21 to 4.25e-28) offer compelling statistical support for rejecting the non-stationarity null hypothesis.  All test statistics easily surpass the critical thresholds, even though JNJ exhibits somewhat weaker stationarity than the other series.  Although this unusual similarity calls for confirmation of potential data processing artifacts, the identical results for PG, S&P 500, and VIX indicate that these return series share similar stationarity properties. These findings confirm that the return series can be appropriately analyzed using standard time series techniques without requiring differencing, making them suitable for VAR modeling, Granger causality tests, and other analyses that assume stationarity. The results validate the use of these financial time series for further econometric modeling while highlighting the importance of always verifying stationarity before conducting time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ad54b-8368-4849-a4b5-4d754a6c1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "dataset.to_csv('dataset.csv') #Its alreadt a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76ffc3-0ba9-4cd1-8a46-44bb47298073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "import statsmodels.tsa.api as smt\n",
    "# create a new variable called model and call the VAR method from the smt package.\n",
    "dataset = dataset.dropna()\n",
    "model = smt.VAR(dataset)\n",
    "res = model.fit(maxlags=2)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d321e-5102-4a65-b397-37d361aaa92b",
   "metadata": {},
   "source": [
    "The dynamic relationships between Cisco (CSCO), T-Mobile (TMUS), the S&P 500 (^GSPC), and the VIX index are examined by this Vector Autoregression (VAR) model.  Its leadership role in price discovery is confirmed by the results, which show an asymmetric relationship structure: the broader market (^GSPC) significantly influences individual stocks, especially TMUS (p=0.001), but not the other way around.  Strong autocorrelation (p<0.001) and a predictable response to lagged market returns (p=0.027) are characteristics of volatility (VIX), which exhibits the \"leverage effect\" as evidenced by the -0.77 residual correlation between ^GSPC and VIX.  Telecom's inherent volatility is reflected in the distinct sector differences, with TMUS exhibiting stronger own-lag effects (β=-0.078, p=0.002) than CSCO.  It is noteworthy that although market fluctuations affect stocks and volatility, the opposite effects are statistically less pronounced—CSCO's returns have little bearing on other factors. The model fits well (AIC: -26.14), with residuals confirming expected relationships, particularly the strong negative linkage between market returns and volatility changes. These findings validate VAR's utility for capturing the multi-directional dependencies in this system, where market performance drives individual stocks and volatility reactions more than the reverse, offering actionable insights for portfolio and risk management strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06b596-f838-44e5-a551-49d679662c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9\n",
    "# Select optimal lag order\n",
    "res = model.select_order(maxlags=20)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0870b36-c2e0-42f6-a052-1f6d0c3a2cde",
   "metadata": {},
   "source": [
    "The BIC recommends one lag, the AIC recommends four. I'll use two lags to balance parsimony and model fit. Choosing the right lag order is essential because too many lags overfit and weaken forecasting power, while too few lags miss significant dynamics. Additionally, the LR test confirms that at least two lags are significant. For the improved model to capture short-term dynamics without being overly complicated, I'll move forward with two lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d73f09-33bd-423a-a8e3-752e86331343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10\n",
    "#Correct number of lags = 10\n",
    "model = smt.VAR(dataset)\n",
    "res = model.fit(maxlags=10) \n",
    "\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce8a03-5b5f-4a1a-b7da-b4a1215e3131",
   "metadata": {},
   "source": [
    "We assessed several information criteria, including AIC, BIC, FPE, and HQIC, in order to choose the best lag order for the VAR model. Each criterion penalizes the addition of needless lags while striking a balance between model fit and complexity.\n",
    "\n",
    "The Akaike Information Criterion (AIC), which reaches its minimum value at this point, recommends a lag order of 10 based on our results. Lag 10 is also supported by the Final Prediction Error (FPE) criterion. However, lag 1 is chosen as the ideal by the Bayesian Information Criterion (BIC), which applies a greater penalty for model complexity. Lag 4 is recommended by the Hannan-Quinn Information Criterion (HQIC).\n",
    "\n",
    "Since AIC and FPE prioritize model fit and minimize forecast error — which is crucial for time series prediction — lag 10 is appropriate in this context. Additionally, the close values of AIC beyond lag 10 indicate that higher lags provide little extra explanatory power. Therefore, we proceed with lag 10 for our VAR model.\n",
    "\n",
    "Determining the lag order is essential because an incorrect selection can result in biased or inefficient parameter estimates. Too few lags may omit significant dynamics (underfitting), while too many lags may overfit the model and reduce forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d15b6-467b-4ed1-a1d4-b02b9d741922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11\n",
    "irf = res.irf(20) # create a variable called irf and assign to it the following: call the res variable and on it the irf() method\n",
    "fig = irf.plot()\n",
    "\n",
    "fig.set_dpi(300)# call the fig variable\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de76796-6f47-46e9-8f48-37e75467fb92",
   "metadata": {},
   "source": [
    "The impulse response plots illustrate the long-term effects of a shock to one variable on other variables as well. A shock to CSCO has a powerful and instantaneous positive impact on it, but it fades away in two short periods, suggesting little long-term impact. There appears to be limited spillover as the effect of CSCO on other variables, such as TMUS, GSPC, and VIX, is slight and quickly diminishes.  In CSCO and GSPC, TMUS shocks cause a moderate response that stabilizes after a short while, indicating localized effects.  Shocks to the GSPC (S&P 500 Index) generate notable reactions on their own and have observable impacts on CSCO and TMUS, demonstrating the index's market-wide influence. VIX shocks have substantial effects on TMUS and GSPC, highlighting the role of market volatility, while their impact on CSCO is minimal, indicating its relative resistance to market uncertainty. The self-response of ^VIX shows a sharp spike followed by a rapid return to stability, which is typical behavior for volatility indices. Overall, the impulse responses suggest that firm-specific shocks (CSCO and TMUS) have limited effects on the broader market, while market-wide shocks (GSPC and VIX) have a more significant and persistent impact on individual firms and the system as a whole. Most responses return to zero within 10 periods, indicating that the shocks have short-term effects and the system returns to equilibrium over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca341b43-29d6-4698-a3db-697ab1bfc49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=10 \n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(dataset, variables, test='ssr_chi2test', verbose=False):    \n",
    "   \n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(dataset[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(dataset, variables = dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035a036-0a4a-42c9-8ccf-315bb75062f8",
   "metadata": {},
   "source": [
    "The findings display the variables' variance decomposition, which shows the percentage of forecast error variance attributable to shocks to each variable.  With its own shocks accounting for 100% of its variance, CSCO is incredibly self-explanatory, indicating little impact from outside sources. Although CSCO accounts for 13.58% of TMUS, which is also primarily explained by its own shocks (100%), there is some interdependence. Due to its market-wide nature, the GSPC (S&P 500 Index) is mostly impacted by its own shocks (100%), with CSCO and TMUS making minor contributions. The volatility index, or VIX, is entirely self-driven, with only a small contribution from TMUS (0.77%) and CSCO (6.96%). Overall, the findings indicate that ~GSPC and ^VIX are more autonomous and motivated by their own shocks, whereas CSCO and TMUS appear to be somewhat connected. The minimal cross-variable influences indicate that firm-specific shocks have limited impact on broader market indicators in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efba4a-717d-47dd-a55b-e647f585d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q13\n",
    "\n",
    "ticker = ['XRP-USD']\n",
    "crypto = yf.download(ticker, start='2019-01-01', end = '2024-12-31')['Close']\n",
    "\n",
    "#Next we calculate the first differenced log returns\n",
    "crypto_returns = np.log(crypto / crypto.shift(1))\n",
    "\n",
    "#Remove missing values\n",
    "crypto_returns = crypto_returns.dropna(how='any')\n",
    "\n",
    "#Transform it to a dataframe\n",
    "crypto_returns = pd.DataFrame(crypto_returns)\n",
    "\n",
    "#examine the first 5 and last 5 rows of the data\n",
    "print(crypto_returns.head(5))\n",
    "print(crypto_returns.tail(5))\n",
    "\n",
    "#Plot the crypto returns\n",
    "crypto_returns.plot(title='XRP-USD return rate over time', figsize=(16, 8))\n",
    "plt.show()\n",
    "\n",
    "#Save to a csv file\n",
    "crypto_returns.to_csv('crypto_returns.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecea44-2384-4275-801c-4d4cbf796a6a",
   "metadata": {},
   "source": [
    "The plot displays the XRP-USD return rate from 2019 to 2024.  The wide variations in return rates show how volatile the cryptocurrency market is.  Notable increases and decreases are a reflection of how the market responds to significant news or events, especially around 2021 when there is a lot of volatility.  The choice of XRP-USD was made because of its widespread use in cross-border payments, high liquidity, and reputation as one of the most popular cryptocurrencies because of its affiliation with Ripple Labs.  Researching XRP yields important information about investor sentiment, the risk involved in trading digital currencies, and the behavior of crypto assets.  Furthermore, because of its volatility pattern, it is a good fit for financial modeling, forecasting, and risk management analysis—all of which are critical to the development of investment strategies and financial data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18630ca2-697d-4b26-8afd-3117658c4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacff974-29f5-4430-aae1-ef416733109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q14\n",
    "import statsmodels.api as sm\n",
    "data = sm.add_constant(crypto_returns)\n",
    "print(data)\n",
    "# create a variable called results and assign to it the following: call the sm package then the OLS() method\n",
    "results = sm.OLS(crypto_returns, data['const']).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1586f-85b6-487b-9a9c-c72240051f0d",
   "metadata": {},
   "source": [
    "A simple OLS regression was run as a benchmark. The R-squared is virtually zero, implying the mean return is not statistically explained by the constant term. The residuals reflect the deviation of observed returns from the fitted constant return. The Jarque-Bera test confirms non-normality (JB-statistic = 40450, p < 0.01), supporting the need for volatility modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf7398-4c9c-45c1-abcc-abcd0d28fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = results.resid\n",
    "resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a59fbd-e00a-4bc5-b147-1ac3a9557a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for the residuals\n",
    "plot = plt.figure(figsize=(9, 5), dpi=100)\n",
    "plt.hist(resid, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3d9b9-9cba-418a-be38-3bc67e0b7ab1",
   "metadata": {},
   "source": [
    "The histogram of XRP-USD returns shows a leptokurtic distribution, most returns cluster tightly around zero with fat tails. This is typical for financial returns, highlighting high volatility and extreme movements more frequently than would be expected in a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec155c-1b47-4fb4-b085-c6cb6f4de91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we test the residuals for ARCH effects\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "res = het_arch(resid, nlags=5) \n",
    "name = ['lm','lmpval','fval','fpval']    \n",
    "lzip(name,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae2d22-31bb-4807-a0b1-855270f33fca",
   "metadata": {},
   "source": [
    "The Lagrange Multiplier (LM) test for ARCH effects yields a test statistic of 77.22 with a p-value of 3.20e-15, rejecting the null hypothesis of no ARCH effect. This provides statistical evidence of volatility clustering, a key justification for using a GARCH model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd2969-b537-4258-8c2f-dc6a884c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46668750-2b62-42e1-b95f-21b45dc8a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the GARCH model (1, 1)\n",
    "from arch import arch_model\n",
    "gm = arch_model(crypto_returns, p = 1, q = 1, vol = 'GARCH', dist = 'normal')\n",
    "gm_result = gm.fit()\n",
    "# Print the summary of gm_results \n",
    "gm_result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4611b-1d24-45b9-8e63-377db3399bd6",
   "metadata": {},
   "source": [
    "I fit a GARCH(1,1) model to the return series after confirming ARCH effects.  Today's volatility is dependent on both past squared residuals (ARCH term) and past volatility (GARCH term), illustrating how the model accounts for time-varying volatility.  The volatility plot shows calm intervals followed by spikes in volatility, which is in line with how the market responds to macroeconomic shocks or news about cryptocurrencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360df33-2c66-4b25-a77d-0d9d9b82d2ce",
   "metadata": {},
   "source": [
    "Overall, the results confirm that XRP-USD returns exhibit volatility clustering and non-normality, making the GARCH(1,1) model appropriate for modeling its volatility. This model helps forecast future volatility, critical for risk management and derivative pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad6b73-20e5-43c4-8da5-0f4fee98f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q15\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Download the daily adjusted close values for the period 2005-2024 for your chosen index.\n",
    "index = ['^GSPC']\n",
    "ID_share = yf.download(index, start='2005-1-1', end='2024-12-31')['Close']\n",
    "ID_share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfff6a8-7a79-47bc-b0f7-a48fa16b0721",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "- Downloaded daily adjusted closing prices for S&P 500 (^GSPC)\n",
    "- Time period: January 2005 - December 2024\n",
    "- Sample includes 5,040 trading days\n",
    "- Data verified for completeness (no missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604a609-0219-4098-9872-4ca652cf1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the S&P 500 prices so you can visually inspect the data.\n",
    "plot = plt.figure(figsize=(16, 8), dpi=100)\n",
    "plt.plot(ID_share, color = 'blue')\n",
    "plt.ylabel('S&P 500 index price since 2005')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Returns over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fc5b0-0ced-46b3-ba7c-3f25066b3aac",
   "metadata": {},
   "source": [
    "Three distinct market phases can be seen in the S&P 500 price trajectory from 2005 to 2024: (1) stability prior to 2008 (roughly 1,200–1,500), (2) post-crisis recovery (2009–2019) with steady growth, and (3) pandemic-era volatility (2020–2024) that drove prices to all-time highs (roughly 6,000). The 2020 COVID crash (-34% in 23 days), the 2008 collapse (-45% from peak), and the stimulus-driven rally that followed are notable turning points. A crucial context for assessing intra-week patterns is provided by the chart's logarithmic progression, which shows how early-period gains (such as the +30% gained from 2005 to 2007) become statistically marginal in later years. According to this macro view, there may be short-term anomalies, but they are part of a larger long-term growth trend that averages about 7.2% per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09734a-8f1e-4a62-a347-e6fea8d40523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage change in prices.\n",
    "index_return=ID_share.pct_change()\n",
    "index_return=pd.DataFrame(index_return.dropna())\n",
    "index_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04d40b-3588-4abc-8a19-bb0930598e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We exclude 2020 onwards due to covid-19, so it doesn't interfere with seasonality\n",
    "index_return = index_return['1984' : '2019']\n",
    "\n",
    "# import calendar \n",
    "import calendar\n",
    "\n",
    "index_return['Weekday'] = index_return.index.weekday.values\n",
    "\n",
    "# This lambda function converts each numeric weekday value in the \"Weekday\" column of the index_ret DataFrame to its corresponding day \n",
    "index_return['Weekday'] = index_return['Weekday'].apply(lambda x: calendar.day_name[x])\n",
    "index_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a645c71-a52f-4802-9c22-6b02c83a365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling the Weekday effect\n",
    "# We first estimate the mean return for each weekday\n",
    "\n",
    "index_weekday_mean = index_return.groupby('Weekday').mean()\n",
    "index_weekday_mean = index_weekday_mean.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'])\n",
    "index_weekday_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f03435-da47-45c9-809d-441226ed7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the returns per weekdays by creating a barplot of the average return for each week day\n",
    "x = index_return['Weekday']\n",
    "y = 100*index_weekday_mean['^GSPC']\n",
    "\n",
    "sns.set_palette('pastel')\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax = sns.barplot(x=index_weekday_mean.index, y=y, data=index_weekday_mean)\n",
    "ax = plt.ylabel('Return (%)', size=15)\n",
    "ax = plt.title('Average daily return for each weekday in 2005 through 2021', size=20)\n",
    "plt.savefig('gspc1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f16dd-1dbe-48cd-889e-ca1bfe9f7e58",
   "metadata": {},
   "source": [
    "The average daily returns for the S&P 500 index on various weekdays from 2005 to 2021 are displayed in the bar chart. According to the data, Monday was the only weekday with a marginally negative average return of -0.01%, while Tuesday produced the highest average return at 0.04%. Friday's performance was neutral at about 0.00%, while Wednesday and Thursday, which are midweek days, displayed modest positive returns of 0.01–0.02%.\n",
    "\n",
    "The observed 0.05 percentage point differential between Tuesday's peak performance and Monday's underperformance represents a relatively small effect size when considered against the index's typical daily volatility. This minor variation suggests that any apparent weekday patterns may not be economically significant for practical trading strategies. The tight clustering of returns near the zero baseline across all weekdays further reinforces the notion that these differences are likely attributable to normal market fluctuations rather than any persistent calendar effect.\n",
    "\n",
    "These findings are consistent with the efficient market hypothesis, which posits that such predictable patterns would be quickly identified and arbitraged away in liquid markets like the S&P 500. The results contrast with some earlier studies that identified more pronounced weekday effects, potentially indicating that such anomalies have diminished as market efficiency has improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a86a3-6c0c-4d33-a211-b939267db29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Scatter plot of all observations to see the variations for each weekday\n",
    "days_mapping = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5}\n",
    "x = index_return['Weekday'].map(days_mapping)\n",
    "y = 100 * index_return['^GSPC']\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(x, y, color='green', linewidths=1, marker='x', s=30)\n",
    "plt.xlabel('Weekday (1=Monday, ..., 5=Friday)')\n",
    "plt.ylabel('Return (%)')\n",
    "plt.title('Daily returns for each weekday for 2005 through 2024')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d0d14-0cf9-433d-9c7c-ca43dc4eef62",
   "metadata": {},
   "source": [
    "With returns ranging from roughly -5% to +5% on all weekdays, the scatter plot of daily returns shows an incredibly symmetrical distribution centered around zero. Maximum gains and losses happen equally frequently on all weekdays, and the data does not reveal any patterns in extreme returns. More than 90% of trading days are represented by the densest cluster of points, which is found within the ±2% band for every weekday. The quantitative conclusions that weekday effects are statistically insignificant in contemporary markets are strongly supported by this visual evidence. The conventional \"weekend effect\" theory is especially refuted by the full overlap in distributions between Mondays (1) and Fridays (5). These observations collectively suggest that daily return patterns are dominated by market-wide factors rather than calendar-based anomalies, reinforcing the efficiency of contemporary equity markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af14ce-cc8c-44a6-9169-4d1bc48c8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatility\n",
    "weekday_vol =index_return[['Weekday', '^GSPC']].groupby('Weekday').std()\n",
    "\n",
    "print(weekday_vol)\n",
    "print(index_return[['Weekday', '^GSPC']].groupby('Weekday').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a014d-e9ed-441e-97c6-889ad3a806ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test\n",
    "\n",
    "from functools import partial\n",
    "import operator\n",
    "average_return = index_weekday_mean['^GSPC'].mean()\n",
    "ttest_func = partial(stats.ttest_1samp, popmean=average_return)\n",
    "ttest_results = index_return.groupby('Weekday')['^GSPC'].apply(ttest_func)\n",
    "result_df = pd.DataFrame.from_records(map(operator.methodcaller('_asdict'), ttest_results),\n",
    "                                        index=ttest_results.index)\n",
    "result_df['df'] = index_return.groupby('Weekday')['^GSPC'].count() - 1\n",
    "formatted_results = (\"t-statistic: \" + result_df['statistic'].round(2).astype(str) +\n",
    "                     \", p-value: \" + result_df['pvalue'].round(4).astype(str) +\n",
    "                     \", degrees of freedom: \" + result_df['df'].astype(str))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3dda0-facc-48af-8d5c-99a537abfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_return['Year'] = index_return.index.year\n",
    "index_mean = index_return.groupby(['Year', 'Weekday'], as_index=False).mean()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "index_mean['Weekday'] = pd.Categorical(index_mean['Weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "palette = {\n",
    "    'Monday': 'red', \n",
    "    'Tuesday': 'blue', \n",
    "    'Wednesday': 'green', \n",
    "    'Thursday': 'orange', \n",
    "    'Friday': 'purple'\n",
    "}\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=index_mean,\n",
    "    x=\"Weekday\",\n",
    "    y=\"^GSPC\",\n",
    "    hue=\"Weekday\",     \n",
    "    kind=\"bar\",\n",
    "    palette=palette,\n",
    "    col=\"Year\",        \n",
    "    col_wrap=4,        \n",
    "    height=3,\n",
    "    aspect=1.33\n",
    ")\n",
    "\n",
    "g.fig.suptitle('Average S&P500 Returns for Each Weekday by Year from 2005 through 2019', fontsize=20)\n",
    "plt.tight_layout()\n",
    "g.fig.subplots_adjust(top=0.93)\n",
    "plt.savefig('gspc3.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda7a87-4227-407c-bc5b-99205bb33c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result_df.csv', index=False) # Save as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3663-629a-4033-b741-7e68a6abf22f",
   "metadata": {},
   "source": [
    "There is some evidence of day-of-the-week effects, according to the analysis. The average returns on Monday are the lowest (-0.0004), but they are not statistically different from those on Friday (reference). Returns are highest on Wednesday (0.0006, p<0.1). Weak overall effects are suggested by the borderline (p=0.11) F-test for the joint significance of all weekday dummies. This pattern is visualized in the plot. These findings are consistent with research showing that calendar anomalies weaken as markets get more efficient. The continued slight underperformance on Mondays could be the result of information accumulation over the weekend. Although additional analysis using subperiods may uncover time-varying patterns, the results indicate limited utility in timing investments based on weekdays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
